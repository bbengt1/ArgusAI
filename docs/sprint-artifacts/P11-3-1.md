# Story P11-3.1: Implement MCPContextProvider MVP with Feedback Context

Status: done

## Story

As a system,
I want to include user feedback history in AI prompts,
so that the AI learns from corrections and improves accuracy.

## Acceptance Criteria

1. AC-3.1.1: MCPContextProvider class created in backend/app/services/
2. AC-3.1.2: Provider gathers recent feedback (last 50 items)
3. AC-3.1.3: Camera-specific accuracy stats included
4. AC-3.1.4: Common corrections summarized
5. AC-3.1.5: Context formatted for AI prompt injection
6. AC-3.1.6: Fail-open design - AI works if context fails

## Tasks / Subtasks

- [x] Task 1: Create MCPContextProvider class structure (AC: 3.1.1)
  - [x] 1.1: Create `backend/app/services/mcp_context.py` with class skeleton
  - [x] 1.2: Define `AIContext`, `FeedbackContext` dataclasses
  - [x] 1.3: Implement `get_context(camera_id, event_time, entity_id)` method signature
  - [x] 1.4: Implement `format_for_prompt(context)` method signature
  - [x] 1.5: Add singleton pattern with `get_mcp_context_provider()` function

- [x] Task 2: Implement feedback history query (AC: 3.1.2, 3.1.3)
  - [x] 2.1: Create `_get_feedback_context(camera_id)` method
  - [x] 2.2: Query last 50 EventFeedback records joined with Event by camera_id
  - [x] 2.3: Calculate accuracy rate (positive / total)
  - [x] 2.4: Handle case of no feedback (return None for accuracy_rate)
  - [x] 2.5: Order by created_at DESC to get most recent first

- [x] Task 3: Extract common correction patterns (AC: 3.1.4)
  - [x] 3.1: Create `_extract_common_patterns(corrections: list[str])` helper
  - [x] 3.2: Tokenize and count keywords from correction text
  - [x] 3.3: Return top 3 most common patterns
  - [x] 3.4: Extract recent negative feedback reasons (last 5 with text)

- [x] Task 4: Create context formatting for prompts (AC: 3.1.5)
  - [x] 4.1: Implement `format_for_prompt(context: AIContext)` method
  - [x] 4.2: Format accuracy as percentage (e.g., "Previous accuracy: 85%")
  - [x] 4.3: Format common corrections as comma-separated list
  - [x] 4.4: Return empty string if no context available

- [x] Task 5: Add fail-open error handling (AC: 3.1.6)
  - [x] 5.1: Wrap `_get_feedback_context` in try/except returning None on error
  - [x] 5.2: Log warnings with structured metadata for failures
  - [x] 5.3: Return partial AIContext if some components fail
  - [x] 5.4: Use `asyncio.gather(return_exceptions=True)` for parallel gathering

- [x] Task 6: Write unit tests (AC: all)
  - [x] 6.1: Test feedback context gathering with seeded data
  - [x] 6.2: Test accuracy calculation (positive, negative, mixed)
  - [x] 6.3: Test common pattern extraction
  - [x] 6.4: Test prompt formatting output
  - [x] 6.5: Test fail-open behavior (simulate database error)
  - [x] 6.6: Test with no feedback data (graceful handling)
  - [x] 6.7: Target 90%+ coverage for mcp_context.py

## Dev Notes

### Architecture Patterns

This story implements the MCPContextProvider as defined in the tech spec and Phase 11 architecture. The provider is an internal service that gathers context from the database and formats it for AI prompt injection. It follows the fail-open design pattern to ensure AI description generation never fails due to context issues.

**Key Design Decisions (from ADR-P11-003):**
- Start with database-backed context rather than full MCP protocol server
- Target <50ms for uncached context queries
- Cache will be added in Story P11-3.4 (not this story)
- Integrate with existing `context_prompt_service.py` patterns

**Integration Points:**
- Uses existing `EventFeedback` model from `backend/app/models/event_feedback.py`
- Will be called from `context_prompt_service.py` (integration in future story)
- Follows same singleton pattern as other services

### Data Model Reference

The `EventFeedback` model has these relevant fields:
- `event_id`: Links to Event (which has camera_id)
- `camera_id`: Denormalized for efficient queries (Story P4-5.2)
- `rating`: 'helpful' or 'not_helpful'
- `correction`: Optional correction text
- `correction_type`: Specific correction type (e.g., 'not_package')
- `created_at`: Timestamp for ordering

### Query Strategy

```python
# Feedback context query (from tech spec)
query = (
    select(EventFeedback)
    .where(EventFeedback.camera_id == camera_id)
    .order_by(EventFeedback.created_at.desc())
    .limit(50)
)
```

### Project Structure Notes

Files to create:
```
backend/app/services/
└── mcp_context.py          # NEW: MCPContextProvider class

backend/tests/test_services/
└── test_mcp_context.py     # NEW: Unit tests
```

This aligns with the Phase 11 architecture structure at `docs/architecture/phase-11-additions.md`.

### Testing Strategy

Follow existing test patterns in `backend/tests/test_services/`. Use pytest fixtures to seed EventFeedback data. Mock database errors to test fail-open behavior.

### Learnings from Previous Story

**From Story P11-2.6 (Notification Thumbnails) - Status: done**

- **SignedURLService pattern**: Created singleton service in `backend/app/services/signed_url_service.py` - follow similar pattern for MCPContextProvider
- **Test structure**: 21 new tests created, follow pattern in `backend/tests/test_services/test_signed_url_service.py`
- **Error handling**: Implemented graceful fallback handling - apply same pattern for fail-open design
- **All 114 push tests passing** - maintain test coverage

**From Story P11-2.5 (Quiet Hours):**
- Device model pattern at `backend/app/models/device.py`
- Dispatch service query patterns in `backend/app/services/push/dispatch_service.py`

**Integration Points:**
- EventFeedback model is well-established with camera_id denormalization
- Can query directly by camera_id without joining Event table

[Source: docs/sprint-artifacts/P11-2-6.md#Completion-Notes-List]

### References

- [Source: docs/sprint-artifacts/tech-spec-epic-P11-3.md#Detailed-Design]
- [Source: docs/sprint-artifacts/tech-spec-epic-P11-3.md#Acceptance-Criteria]
- [Source: docs/epics-phase11.md#P11-3.1]
- [Source: docs/architecture/phase-11-additions.md#MCP-Context-Provider]
- [Source: docs/architecture/phase-11-additions.md#ADR-P11-003]
- [Source: backend/app/models/event_feedback.py]
- [Source: backend/app/services/context_prompt_service.py]

## Dev Agent Record

### Context Reference

- docs/sprint-artifacts/P11-3-1.context.xml

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References

### Completion Notes List

- Created MCPContextProvider class with dataclasses for AIContext, FeedbackContext, EntityContext, CameraContext, TimePatternContext
- Implemented _get_feedback_context() to query last 50 feedback items by camera_id
- Implemented _extract_common_patterns() with stop-word filtering and word frequency counting
- Implemented format_for_prompt() to generate human-readable context text
- Added fail-open error handling with _safe_get_feedback_context() wrapper
- Implemented singleton pattern with get_mcp_context_provider() and reset_mcp_context_provider()
- Created 25 unit tests covering all acceptance criteria
- All 25 tests passing

### File List

**New Files:**
- backend/app/services/mcp_context.py
- backend/tests/test_services/test_mcp_context.py

## Change Log

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2025-12-26 | Claude | Initial story creation |
| 2.0 | 2025-12-26 | Claude | Implementation complete - all tasks done |
